{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar função para requisitar a etiquetagem do PortService-Br\n",
    "    # Importante observar que, quando a palavra é desconhecida, o PortService-Br não etiqueta adequadamente a palavra\n",
    "    # Palavras desconhecidas incluem nomes próprios, abreviações, palavras com erros de ortografia, e caracteres especiais e sinais de pontuação que não são conhecidos pelo etiquetador Freeling\n",
    "# Criar função para somar as etiquetas por tipo\n",
    "\n",
    "# Criar um DataFrame para ilustrar isso, associado a cada ISBN-10 ou ISBN-13\n",
    "# Criar função para plotar os valores num gráfico usando Matplotlib\n",
    "\n",
    "# Aplicar os procedimentos em strings com stopwords\n",
    "# Remover as stopwords\n",
    "# Aplicar os procedimentos em strings sem stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestPosTagging(text):\n",
    "    try:\n",
    "        payload = {'texto': str(text)}\n",
    "        response = requests.get('http://portservice.pythonanywhere.com/analise/morfossintatica/freeling', params=payload).json()\n",
    "    except:\n",
    "        print(text)\n",
    "    return response\n",
    "\n",
    "def sumDifferentTags(jsonObject):    \n",
    "    tagsDictionary = {\n",
    "        'A': 0,\n",
    "        'C': 0,\n",
    "        'D': 0,\n",
    "        'N': 0,\n",
    "        'P': 0,\n",
    "        'R': 0,\n",
    "        'S': 0,\n",
    "        'V': 0,\n",
    "        'Z': 0,\n",
    "        'W': 0,\n",
    "        'I': 0,\n",
    "        'F': 0,\n",
    "        'UNKNOWN': 0\n",
    "    }\n",
    "    \n",
    "    for word in jsonObject:\n",
    "        if word['lemma'] != '<unknown>':\n",
    "            if word['poss'][0] in tagsDictionary:\n",
    "                tagsDictionary[word['poss'][0]] += 1\n",
    "            else:\n",
    "                tagsDictionary['UNKNOWN'] += 1\n",
    "        else:\n",
    "            tagsDictionary['UNKNOWN'] += 1\n",
    "\n",
    "    return tagsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 6, 'C': 7, 'D': 21, 'N': 23, 'P': 19, 'R': 11, 'S': 15, 'V': 31, 'Z': 0, 'W': 0, 'I': 0, 'F': 0, 'UNKNOWN': 0}\n",
      "\n",
      "\n",
      "[{'token': 'resgate', 'lemma': 'resgate', 'poss': 'NCMS'}, {'token': 'foi', 'lemma': 'ir', 'poss': 'VMI'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'primeira', 'lemma': 'primeiro', 'poss': 'AO0'}, {'token': 'palavra', 'lemma': 'palavra', 'poss': 'NCFS'}, {'token': 'que', 'lemma': 'que', 'poss': 'PR0'}, {'token': 'ouvi', 'lemma': 'ouvir', 'poss': 'VMI'}, {'token': 'quando', 'lemma': 'quando', 'poss': 'RG'}, {'token': 'os', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'tiros', 'lemma': 'tiro', 'poss': 'NCMP'}, {'token': 'cessaram', 'lemma': 'cessar', 'poss': 'VMI'}, {'token': 'ele', 'lemma': 'ele', 'poss': 'PP3'}, {'token': 'está', 'lemma': 'estar', 'poss': 'VMI'}, {'token': 'ferido', 'lemma': 'ferir', 'poss': 'VMP'}, {'token': 'tire', 'lemma': 'tirar', 'poss': 'VMS'}, {'token': 'o', 'lemma': 'o', 'poss': 'PD0'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'aqui', 'lemma': 'aqui', 'poss': 'RG'}, {'token': 'foi', 'lemma': 'ir', 'poss': 'VMI'}, {'token': 'o', 'lemma': 'o', 'poss': 'PD0'}, {'token': 'que', 'lemma': 'que', 'poss': 'PR0'}, {'token': 'ouvi', 'lemma': 'ouvir', 'poss': 'VMI'}, {'token': 'quando', 'lemma': 'quando', 'poss': 'RG'}, {'token': 'fui', 'lemma': 'ir', 'poss': 'VMI'}, {'token': 'levado', 'lemma': 'levar', 'poss': 'VMP'}, {'token': 'para', 'lemma': 'para', 'poss': 'SPS'}, {'token': 'cima', 'lemma': 'cima', 'poss': 'NCFS'}, {'token': 'arranque', 'lemma': 'arrancar', 'poss': 'VMS'}, {'token': 'isso', 'lemma': 'isso', 'poss': 'PD0'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'ele', 'lemma': 'ele', 'poss': 'PP3'}, {'token': 'foi', 'lemma': 'ir', 'poss': 'VMI'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'última', 'lemma': 'último', 'poss': 'AO0'}, {'token': 'coisa', 'lemma': 'coisa', 'poss': 'NCFS'}, {'token': 'que', 'lemma': 'que', 'poss': 'PR0'}, {'token': 'ouvi', 'lemma': 'ouvir', 'poss': 'VMI'}, {'token': 'antes', 'lemma': 'antes', 'poss': 'RG'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'me', 'lemma': 'me', 'poss': 'PP1'}, {'token': 'render', 'lemma': 'render', 'poss': 'VMN'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'escuridão', 'lemma': 'escuridão', 'poss': 'NCFS'}, {'token': 'uma', 'lemma': 'um', 'poss': 'DI0'}, {'token': 'década', 'lemma': 'década', 'poss': 'NCFS'}, {'token': 'depois', 'lemma': 'depois', 'poss': 'RG'}, {'token': 'e', 'lemma': 'e', 'poss': 'CC'}, {'token': 'essas', 'lemma': 'esse', 'poss': 'DD0'}, {'token': 'palavras', 'lemma': 'palavra', 'poss': 'NCFP'}, {'token': 'ainda', 'lemma': 'ainda', 'poss': 'RG'}, {'token': 'permeiam', 'lemma': 'permear', 'poss': 'VMI'}, {'token': 'meus', 'lemma': 'meu', 'poss': 'DP1'}, {'token': 'pensamentos', 'lemma': 'pensamento', 'poss': 'NCMP'}, {'token': 'antes', 'lemma': 'antes', 'poss': 'RG'}, {'token': 'elas', 'lemma': 'ele', 'poss': 'PP3'}, {'token': 'me', 'lemma': 'me', 'poss': 'PP1'}, {'token': 'causavam', 'lemma': 'causar', 'poss': 'VMI'}, {'token': 'medo', 'lemma': 'medo', 'poss': 'NCMS'}, {'token': 'mas', 'lemma': 'mas', 'poss': 'CC'}, {'token': 'aprendi', 'lemma': 'aprender', 'poss': 'VMI'}, {'token': 'a', 'lemma': 'a', 'poss': 'SPS'}, {'token': 'lidar', 'lemma': 'lidar', 'poss': 'VMN'}, {'token': 'com', 'lemma': 'com', 'poss': 'SPS'}, {'token': 'ele', 'lemma': 'ele', 'poss': 'PP3'}, {'token': 'transformei', 'lemma': 'transformar', 'poss': 'VMI'}, {'token': 'o', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'medo', 'lemma': 'medo', 'poss': 'NCMS'}, {'token': 'em', 'lemma': 'em', 'poss': 'SPS'}, {'token': 'ódio', 'lemma': 'ódio', 'poss': 'NCMS'}, {'token': 'em', 'lemma': 'em', 'poss': 'SPS'}, {'token': 'uma', 'lemma': 'um', 'poss': 'DI0'}, {'token': 'motivação', 'lemma': 'motivação', 'poss': 'NCFS'}, {'token': 'o', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'combustível', 'lemma': 'combustível', 'poss': 'NCMS'}, {'token': 'que', 'lemma': 'que', 'poss': 'PR0'}, {'token': 'precisava', 'lemma': 'precisar', 'poss': 'VMI'}, {'token': 'e', 'lemma': 'e', 'poss': 'CC'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'justificativa', 'lemma': 'justificativa', 'poss': 'NCFS'}, {'token': 'perfeita', 'lemma': 'perfeito', 'poss': 'AQ0'}, {'token': 'para', 'lemma': 'para', 'poss': 'SPS'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'escuridão', 'lemma': 'escuridão', 'poss': 'NCFS'}, {'token': 'dentro', 'lemma': 'dentro', 'poss': 'RG'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'mim', 'lemma': 'mim', 'poss': 'PP1'}, {'token': 'hoje', 'lemma': 'hoje', 'poss': 'RG'}, {'token': 'eu', 'lemma': 'eu', 'poss': 'PP1'}, {'token': 'tenho', 'lemma': 'ter', 'poss': 'VMI'}, {'token': 'um', 'lemma': 'um', 'poss': 'DI0'}, {'token': 'plano', 'lemma': 'plano', 'poss': 'NCMS'}, {'token': 'e', 'lemma': 'e', 'poss': 'CC'}, {'token': 'não', 'lemma': 'não', 'poss': 'RN'}, {'token': 'vou', 'lemma': 'ir', 'poss': 'VMI'}, {'token': 'desviar', 'lemma': 'desviar', 'poss': 'VMN'}, {'token': 'nem', 'lemma': 'nem', 'poss': 'CC'}, {'token': 'que', 'lemma': 'que', 'poss': 'CS'}, {'token': 'isso', 'lemma': 'isso', 'poss': 'PD0'}, {'token': 'signifique', 'lemma': 'significar', 'poss': 'VMS'}, {'token': 'silenciar', 'lemma': 'silenciar', 'poss': 'VMN'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'única', 'lemma': 'único', 'poss': 'AQ0'}, {'token': 'voz', 'lemma': 'voz', 'poss': 'NCFS'}, {'token': 'capaz', 'lemma': 'capaz', 'poss': 'AQ0'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'acalmar', 'lemma': 'acalmar', 'poss': 'VMN'}, {'token': 'os', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'demônios', 'lemma': 'demônio', 'poss': 'NCMP'}, {'token': 'em', 'lemma': 'em', 'poss': 'SPS'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'minha', 'lemma': 'meu', 'poss': 'DP1'}, {'token': 'cabeça', 'lemma': 'cabeça', 'poss': 'NCFS'}, {'token': 'danificada', 'lemma': 'danificar', 'poss': 'VMP'}, {'token': 'eles', 'lemma': 'ele', 'poss': 'PP3'}, {'token': 'roubaram', 'lemma': 'roubar', 'poss': 'VMI'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'minha', 'lemma': 'meu', 'poss': 'DP1'}, {'token': 'vida', 'lemma': 'vida', 'poss': 'NCFS'}, {'token': 'e', 'lemma': 'e', 'poss': 'CC'}, {'token': 'não', 'lemma': 'não', 'poss': 'RN'}, {'token': 'a', 'lemma': 'o', 'poss': 'PP3'}, {'token': 'quero', 'lemma': 'querer', 'poss': 'VMI'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'volta', 'lemma': 'volta', 'poss': 'NCFS'}, {'token': 'meu', 'lemma': 'meu', 'poss': 'DP1'}, {'token': 'único', 'lemma': 'único', 'poss': 'AQ0'}, {'token': 'desejo', 'lemma': 'desejo', 'poss': 'NCMS'}, {'token': 'é', 'lemma': 'ser', 'poss': 'VMI'}, {'token': 'acabar', 'lemma': 'acabar', 'poss': 'VMN'}, {'token': 'com', 'lemma': 'com', 'poss': 'SPS'}, {'token': 'a', 'lemma': 'o', 'poss': 'DA0'}, {'token': 'de', 'lemma': 'de', 'poss': 'SPS'}, {'token': 'eles', 'lemma': 'ele', 'poss': 'PP3'}]\n"
     ]
    }
   ],
   "source": [
    "text = \"resgate foi a primeira palavra que ouvi quando os tiros cessaram ele está ferido tire o daqui foi o que ouvi quando fui levado para cima arranque isso dele foi a última coisa que ouvi antes de me render a escuridão uma década depois e essas palavras ainda permeiam meus pensamentos antes elas me causavam medo mas aprendi a lidar com ele transformei o medo em ódio em uma motivação o combustível que precisava e a justificativa perfeita para a escuridão dentro de mim hoje eu tenho um plano e não vou desviar nem que isso signifique silenciar a única voz capaz de acalmar os demônios na minha cabeça danificada eles roubaram a minha vida e não a quero de volta meu único desejo é acabar com a deles\"\n",
    "response = requestPosTagging(text)\n",
    "dictionary = sumDifferentTags(response)\n",
    "print(dictionary)\n",
    "print('\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'C:/Users/LuisRicardoFerraz/Documents/personal/projects/book-similarity/test/tutorial/tutorial/spiders/synopsis-preprocessing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CSV file to a DataFrame\n",
    "df = pd.read_csv(filePath+'dataSetComSinopsesLimpas.csv')\n",
    "df = df.drop(['Unnamed: 0'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "newdf = df[['isbn-10','isbn-13','titulo','sinopse']]\n",
    "initialList = []\n",
    "i = 0\n",
    "while i < newdf.shape[0]:\n",
    "    initialList.append(0)\n",
    "    i+=1\n",
    "array = ['adjective','conjunction','determiner','noun','pronoun','adverb','adposition','verb','number','date','interjection','punctuation','unknown']\n",
    "for column in array:\n",
    "    newdf[column] = initialList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\LuisRicardoFerraz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 10:\n",
    "    text = newdf.sinopse.iloc[i]\n",
    "    variable = requestPosTagging(text)\n",
    "    dictionary = sumDifferentTags(variable)\n",
    "    newdf.adjective.iloc[i] = dictionary.get('A')\n",
    "    newdf.conjunction.iloc[i] = dictionary.get('C')\n",
    "    newdf.determiner.iloc[i] = dictionary.get('D')\n",
    "    newdf.noun.iloc[i] = dictionary.get('N')\n",
    "    newdf.pronoun.iloc[i] = dictionary.get('P')\n",
    "    newdf.adverb.iloc[i] = dictionary.get('R')\n",
    "    newdf.adposition.iloc[i] = dictionary.get('S')\n",
    "    newdf.verb.iloc[i] = dictionary.get('V')\n",
    "    newdf.number.iloc[i] = dictionary.get('Z')\n",
    "    newdf.date.iloc[i] = dictionary.get('W')\n",
    "    newdf.interjection.iloc[i] = dictionary.get('I')\n",
    "    newdf.punctuation.iloc[i] = dictionary.get('F')\n",
    "    newdf.unknown.iloc[i] = dictionary.get('UNKNOWN')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn-10</th>\n",
       "      <th>isbn-13</th>\n",
       "      <th>titulo</th>\n",
       "      <th>sinopse</th>\n",
       "      <th>adjective</th>\n",
       "      <th>conjunction</th>\n",
       "      <th>determiner</th>\n",
       "      <th>noun</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>adverb</th>\n",
       "      <th>adposition</th>\n",
       "      <th>verb</th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>interjection</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8552923262</td>\n",
       "      <td>9788552923268</td>\n",
       "      <td>Tapete Vermelho</td>\n",
       "      <td>uma jovem garota brasileira resolve se aventur...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8552923440</td>\n",
       "      <td>9788552923442</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>resgate foi a primeira palavra que ouvi quand...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8552923157</td>\n",
       "      <td>9788552923152</td>\n",
       "      <td>Cage</td>\n",
       "      <td>cage substantivo cela prisão verbo prender enj...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1408857901</td>\n",
       "      <td>9781408857908</td>\n",
       "      <td>A Court of Wings and Ruin</td>\n",
       "      <td>feyre has returned to the spring court determi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855292336X</td>\n",
       "      <td>9788552923367</td>\n",
       "      <td>Espere Por Mim</td>\n",
       "      <td>quebrado irritado abandonado era assim que ton...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8580418895</td>\n",
       "      <td>9788580418897</td>\n",
       "      <td>Vox</td>\n",
       "      <td>uma distopia atual próxima dos dias de hoje so...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8552923394</td>\n",
       "      <td>9788552923398</td>\n",
       "      <td>Ayra</td>\n",
       "      <td>as histórias de contos de fadas que as criança...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8576865238</td>\n",
       "      <td>9788576865230</td>\n",
       "      <td>Amor Plus Size</td>\n",
       "      <td>maitê passos é uma garota linda de dezessete a...</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8565859150</td>\n",
       "      <td>9788565859158</td>\n",
       "      <td>O Sal da Vida</td>\n",
       "      <td>existe uma forma de leveza e de graça no simpl...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8501401439</td>\n",
       "      <td>9788501401434</td>\n",
       "      <td>Aliança</td>\n",
       "      <td>um thriller sobre a luta secreta entre o bem e...</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      isbn-10        isbn-13                     titulo  \\\n",
       "0  8552923262  9788552923268            Tapete Vermelho   \n",
       "1  8552923440  9788552923442                     Hunter   \n",
       "2  8552923157  9788552923152                       Cage   \n",
       "3  1408857901  9781408857908  A Court of Wings and Ruin   \n",
       "4  855292336X  9788552923367             Espere Por Mim   \n",
       "5  8580418895  9788580418897                        Vox   \n",
       "6  8552923394  9788552923398                       Ayra   \n",
       "7  8576865238  9788576865230             Amor Plus Size   \n",
       "8  8565859150  9788565859158              O Sal da Vida   \n",
       "9  8501401439  9788501401434                    Aliança   \n",
       "\n",
       "                                             sinopse  adjective  conjunction  \\\n",
       "0  uma jovem garota brasileira resolve se aventur...          6            2   \n",
       "1   resgate foi a primeira palavra que ouvi quand...          6            7   \n",
       "2  cage substantivo cela prisão verbo prender enj...          5            6   \n",
       "3  feyre has returned to the spring court determi...          0            0   \n",
       "4  quebrado irritado abandonado era assim que ton...          5            8   \n",
       "5  uma distopia atual próxima dos dias de hoje so...          4            8   \n",
       "6  as histórias de contos de fadas que as criança...          2            9   \n",
       "7  maitê passos é uma garota linda de dezessete a...         11            8   \n",
       "8  existe uma forma de leveza e de graça no simpl...         12           11   \n",
       "9  um thriller sobre a luta secreta entre o bem e...          9            7   \n",
       "\n",
       "   determiner  noun  pronoun  adverb  adposition  verb  number  date  \\\n",
       "0          15    21       12       7          17    16       0     0   \n",
       "1          21    23       19      11          15    31       0     0   \n",
       "2          14    21        9      10          11    26       0     0   \n",
       "3          15     9        7       0           5     8       0     0   \n",
       "4          19    19       20       9          26    30       0     0   \n",
       "5          18    25        5      11          14    25       1     0   \n",
       "6          10    17       17      13          16    26       1     0   \n",
       "7          23    28        3       6          18    16       1     0   \n",
       "8          25    26       14      10          24    17       0     0   \n",
       "9          16    23        6       4          15    15       0     0   \n",
       "\n",
       "   interjection  punctuation  unknown  \n",
       "0             0            0        0  \n",
       "1             0            0        0  \n",
       "2             0            0        2  \n",
       "3             0            0       84  \n",
       "4             0            0        4  \n",
       "5             0            0        5  \n",
       "6             0            0        0  \n",
       "7             0            0        9  \n",
       "8             0            0        3  \n",
       "9             0            0        3  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "['resgate', 'foi', 'a', 'primeira', 'palavra', 'que', 'ouvi', 'quando', 'os', 'tiros', 'cessaram', 'ele', 'está', 'ferido', 'tire', 'o', 'daqui', 'foi', 'o', 'que', 'ouvi', 'quando', 'fui', 'levado', 'para', 'cima', 'arranque', 'isso', 'dele', 'foi', 'a', 'última', 'coisa', 'que', 'ouvi', 'antes', 'de', 'me', 'render', 'a', 'escuridão', 'uma', 'década', 'depois', 'e', 'essas', 'palavras', 'ainda', 'permeiam', 'meus', 'pensamentos', 'antes', 'elas', 'me', 'causavam', 'medo', 'mas', 'aprendi', 'a', 'lidar', 'com', 'ele', 'transformei', 'o', 'medo', 'em', 'ódio', 'em', 'uma', 'motivação', 'o', 'combustível', 'que', 'precisava', 'e', 'a', 'justificativa', 'perfeita', 'para', 'a', 'escuridão', 'dentro', 'de', 'mim', 'hoje', 'eu', 'tenho', 'um', 'plano', 'e', 'não', 'vou', 'desviar', 'nem', 'que', 'isso', 'signifique', 'silenciar', 'a', 'única', 'voz', 'capaz', 'de', 'acalmar', 'os', 'demônios', 'na', 'minha', 'cabeça', 'danificada', 'eles', 'roubaram', 'a', 'minha', 'vida', 'e', 'não', 'a', 'quero', 'de', 'volta', 'meu', 'único', 'desejo', 'é', 'acabar', 'com', 'a', 'deles']\n"
     ]
    }
   ],
   "source": [
    "palavras = word_tokenize(text)\n",
    "print(len(palavras))\n",
    "print(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('portuguese') + list(punctuation))\n",
    "palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "resgate primeira palavra ouvi tiros cessaram ferido tire daqui ouvi levado cima arranque última coisa ouvi antes render escuridão década palavras ainda permeiam pensamentos antes causavam medo aprendi lidar transformei medo ódio motivação combustível precisava justificativa perfeita escuridão dentro mim hoje plano vou desviar signifique silenciar única voz capaz acalmar demônios cabeça danificada roubaram vida quero volta único desejo é acabar\n"
     ]
    }
   ],
   "source": [
    "print(len(palavras_sem_stopwords))\n",
    "final_string = \" \".join(palavras_sem_stopwords)\n",
    "print(final_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
